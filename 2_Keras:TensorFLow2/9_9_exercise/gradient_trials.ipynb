{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import backend as K\n",
    "import numpy as np\n",
    "\n",
    "def grad( y, x ):\n",
    "    return Lambda( lambda z: K.gradients( z[ 0 ], z[ 1 ] ), output_shape = [1] )( [ y, x ] )\n",
    "\n",
    "def network( i ):\n",
    "    a = Lambda(lambda x: K.log( x + 2 ) )( i )\n",
    "    return a\n",
    "\n",
    "fixed_input = Input(shape=(1,))\n",
    "# double = Input(tensor=tf.Variable( [ 2.0 ] ) )\n",
    "unit_activation = tf.keras.initializers.Ones()\n",
    "dense_dummy_layer = keras.layers.Dense(units=1,use_bias=False,\n",
    "                                      kernel_initializer=unit_activation)(fixed_input)\n",
    "# a = network(fixed_input)\n",
    "a = network(dense_dummy_layer)\n",
    "\n",
    "b = grad( a, fixed_input )\n",
    "c = grad( b, fixed_input )\n",
    "# d = grad( c, fixed_input )\n",
    "# e = grad( d, fixed_input )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[1],\n",
    "            [2],\n",
    "            [3]])\n",
    "model = Model( inputs = [ fixed_input], outputs = [ a,b] )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1)            1           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 1)            0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               [(None, 1)]          0           lambda[0][0]                     \n",
      "                                                                 input_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1\n",
      "Trainable params: 1\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[1.0986123],\n",
      "       [1.3862944],\n",
      "       [1.609438 ]], dtype=float32), [array([[0.33333334],\n",
      "       [0.25      ],\n",
      "       [0.2       ]], dtype=float32)]]\n"
     ]
    }
   ],
   "source": [
    "print( model.predict( x, steps = 1 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0986122886681098 0.3333333333333333 -0.1111111111111111\n",
      "1.3862943611198906 0.25 -0.0625\n",
      "1.6094379124341003 0.2 -0.04\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,4):\n",
    "    print(np.log(i+2),1/(i+2),-1/(i+2)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying new things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import backend as K\n",
    "import numpy as np\n",
    "\n",
    "def grad( y, x ):\n",
    "    return Lambda( lambda z: K.gradients( z[ 0 ], z[ 1 ] ))( [ y, x\n",
    "                                                             ] )\n",
    "\n",
    "def network( i ):\n",
    "    a = Lambda(lambda x: K.log( x + 2 ) )( i )\n",
    "    return a\n",
    "\n",
    "fixed_input = Input(shape=(2,))\n",
    "# double = Input(tensor=tf.Variable( [ 2.0 ] ) )\n",
    "unit_activation = tf.keras.initializers.Ones()\n",
    "# dense_dummy_layer = keras.layers.Dense(units=2,use_bias=False,\n",
    "#                                       kernel_initializer=unit_activation)(fixed_input)\n",
    "# a = network(fixed_input)\n",
    "# a = network(dense_dummy_layer)\n",
    "a = Lambda(lambda x: x[:,0]*x[:,1]) (fixed_input)\n",
    "\n",
    "# taking derrivative with respect to secon variable\n",
    "b = keras.layers.Lambda(lambda z: K.gradients(z[0],z[1])) ([a,fixed_input])\n",
    "# c = grad( b, fixed_input )\n",
    "# d = grad( c, fixed_input )\n",
    "# e = grad( d, fixed_input )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[1,4],\n",
    "            [2,5],\n",
    "            [3,6]])\n",
    "model = Model( inputs = [ fixed_input], outputs = [a, b] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 2)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None,)              0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               [(None, 2)]          0           lambda_3[0][0]                   \n",
      "                                                                 input_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 0\n",
      "Trainable params: 0\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 4., 10., 18.], dtype=float32), [array([[4., 1.],\n",
      "       [5., 2.],\n",
      "       [6., 3.]], dtype=float32)]]\n"
     ]
    }
   ],
   "source": [
    "print( model.predict( x, steps = 1 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.09861228866811 0.3333333333333333 -0.1111111111111111\n",
      "3.386294361119891 0.25 -0.0625\n",
      "4.6094379124341005 0.2 -0.04\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,4):\n",
    "    print(np.log(i+2) + i,1/(i+2),-1/(i+2)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the Hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import backend as K\n",
    "import numpy as np\n",
    "\n",
    "def hes( y, x ):\n",
    "    return Lambda( lambda z: tf.hessians( z[ 0 ], z[ 1 ] ), output_shape = [1] )( [ y, x ] )\n",
    "\n",
    "def network( i ):\n",
    "    a = Lambda(lambda x: K.log( x + 2 ) )( i )\n",
    "    return a\n",
    "\n",
    "fixed_input = Input(shape=(1,))\n",
    "# double = Input(tensor=tf.Variable( [ 2.0 ] ) )\n",
    "unit_activation = tf.keras.initializers.Ones()\n",
    "dense_dummy_layer = keras.layers.Dense(units=1,use_bias=False,\n",
    "                                      kernel_initializer=unit_activation)(fixed_input)\n",
    "# a = network(fixed_input)\n",
    "a = network(dense_dummy_layer)\n",
    "\n",
    "b = hes( a, fixed_input )\n",
    "# c = grad( b, fixed_input )\n",
    "# d = grad( c, fixed_input )\n",
    "# e = grad( d, fixed_input )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[1],\n",
    "            [2],\n",
    "            [3]])\n",
    "model = Model( inputs = [ fixed_input], outputs = [b] )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            1           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 1)            0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               [(None, 1, None, 1)] 0           lambda_5[0][0]                   \n",
      "                                                                 input_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1\n",
      "Trainable params: 1\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[[[-0.11111112],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ]]],\n",
      "\n",
      "\n",
      "       [[[ 0.        ],\n",
      "         [-0.0625    ],\n",
      "         [ 0.        ]]],\n",
      "\n",
      "\n",
      "       [[[ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [-0.04      ]]]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "print( model.predict( x, steps = 1 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0986122886681098 0.3333333333333333 -0.1111111111111111\n",
      "1.3862943611198906 0.25 -0.0625\n",
      "1.6094379124341003 0.2 -0.04\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,4):\n",
    "    print(np.log(i+2),1/(i+2),-1/(i+2)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting everything inside a custom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CustomModel, self).__init__()\n",
    "        self.input_layer = Lambda(lambda x: K.log( x+2  ) )\n",
    "\n",
    "    def findGrad(self,func,argm):\n",
    "        return keras.layers.Lambda(lambda x: K.gradients(x[0],x[1])) ([func,argm])\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        Z = self.input_layer(inputs)\n",
    "        Z_1 = self.findGrad(Z,inputs)\n",
    "        #     return self.dense2(x)\n",
    "        #     print(\"\\n\\n\\n this is the answer:\",self.square_layer(inputs))\n",
    "        #     print(\"hre is a break\")\n",
    "        return Z_1\n",
    "\n",
    "\n",
    "custom_model = CustomModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[0.],\n",
    "            [1],\n",
    "            [2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.5       ],\n",
       "        [0.33333334],\n",
       "        [0.25      ]], dtype=float32)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_model.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poisson equation for $x^3$\n",
    "\n",
    "Focus on the findPde layer, input_layer is justa dummy with log in it. IT doesnt matter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CustomModel, self).__init__()\n",
    "        self.input_layer = Lambda(lambda x: K.log( x+2  ) )\n",
    "\n",
    "    def findGrad(self,func,argm):\n",
    "        return keras.layers.Lambda(lambda x: K.gradients(x[0],x[1])[0]) ([func, argm])\n",
    "    \n",
    "    def findPde(self, func, argm):\n",
    "        return keras.layers.Lambda(lambda z: z[0] + 6*z[1]) ([func, argm])\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        Z = self.input_layer(inputs)\n",
    "        Z_1 = self.findGrad(Z,inputs)\n",
    "        #     return self.dense2(x)\n",
    "        #     print(\"\\n\\n\\n this is the answer:\",self.square_layer(inputs))\n",
    "        #     print(\"hre is a break\")\n",
    "        return Z_1, self.findPde(Z_1, inputs)\n",
    "\n",
    "\n",
    "custom_model = CustomModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[0.],\n",
    "            [1],\n",
    "            [2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.uniform(0,3,100).reshape((100,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.20146704,  2.20146702],\n",
       "       [ 1.71498108,  1.71498096],\n",
       "       [ 5.88981819,  5.88981825],\n",
       "       [ 1.10338783,  1.10338783],\n",
       "       [ 9.36182308,  9.36182347],\n",
       "       [ 9.07505512,  9.07505475],\n",
       "       [ 8.39815235,  8.39815301],\n",
       "       [17.51659203, 17.51659367],\n",
       "       [13.56720161, 13.56720128],\n",
       "       [ 9.55588818,  9.55588772],\n",
       "       [ 6.03108692,  6.03108687],\n",
       "       [17.01443291, 17.01443357],\n",
       "       [14.88874817, 14.88874678],\n",
       "       [ 4.36989927,  4.36989905],\n",
       "       [ 0.83081353,  0.83081348],\n",
       "       [11.28807449, 11.28807375],\n",
       "       [ 8.95215893,  8.95215936],\n",
       "       [13.18354416, 13.18354389],\n",
       "       [ 6.21052885,  6.2105288 ],\n",
       "       [18.07797813, 18.07797776],\n",
       "       [14.68864918, 14.68865004],\n",
       "       [ 3.98990107,  3.98990114],\n",
       "       [17.82394409, 17.82394499],\n",
       "       [12.99127388, 12.99127364],\n",
       "       [17.73463631, 17.73463773],\n",
       "       [ 8.8322897 ,  8.83228934],\n",
       "       [11.3368721 , 11.33687302],\n",
       "       [ 2.44668818,  2.44668795],\n",
       "       [ 6.72441816,  6.7244178 ],\n",
       "       [ 6.24107742,  6.24107721],\n",
       "       [13.09203053, 13.09203058],\n",
       "       [14.18494129, 14.18494187],\n",
       "       [ 3.96385217,  3.96385227],\n",
       "       [14.5067997 , 14.50680006],\n",
       "       [10.415452  , 10.41545212],\n",
       "       [ 1.29834545,  1.29834548],\n",
       "       [14.60114193, 14.60114143],\n",
       "       [14.92726326, 14.92726448],\n",
       "       [ 2.11279774,  2.11279788],\n",
       "       [13.96146488, 13.96146519],\n",
       "       [ 9.24106503,  9.2410654 ],\n",
       "       [15.60036755, 15.60036608],\n",
       "       [ 2.48641968,  2.48641966],\n",
       "       [12.63807487, 12.63807457],\n",
       "       [ 6.54883194,  6.54883215],\n",
       "       [ 4.75188541,  4.7518855 ],\n",
       "       [11.30098438, 11.30098372],\n",
       "       [ 5.38327885,  5.38327881],\n",
       "       [ 9.56549931,  9.56549934],\n",
       "       [ 1.40734851,  1.4073485 ],\n",
       "       [ 2.54727221,  2.54727227],\n",
       "       [14.09424019, 14.09424022],\n",
       "       [ 2.94147515,  2.94147517],\n",
       "       [16.44695091, 16.44695115],\n",
       "       [ 4.32614613,  4.32614603],\n",
       "       [10.00768948, 10.00769029],\n",
       "       [ 2.8597734 ,  2.8597734 ],\n",
       "       [ 2.88459468,  2.8845947 ],\n",
       "       [ 1.91404295,  1.91404286],\n",
       "       [14.25287247, 14.25287296],\n",
       "       [11.73621178, 11.73621116],\n",
       "       [ 8.88746834,  8.88746786],\n",
       "       [ 8.14473534,  8.14473535],\n",
       "       [10.14054871, 10.1405484 ],\n",
       "       [12.0235281 , 12.02352838],\n",
       "       [13.04075813, 13.04075737],\n",
       "       [ 5.85480642,  5.85480636],\n",
       "       [ 2.74096012,  2.74095993],\n",
       "       [ 6.74913597,  6.74913617],\n",
       "       [15.07677937, 15.07677976],\n",
       "       [ 3.2789278 ,  3.27892782],\n",
       "       [13.85569   , 13.85569063],\n",
       "       [15.79108429, 15.79108347],\n",
       "       [ 8.4364109 ,  8.43641024],\n",
       "       [11.52519703, 11.52519691],\n",
       "       [14.70347309, 14.703473  ],\n",
       "       [ 7.65174103,  7.65174094],\n",
       "       [ 5.97807884,  5.97807886],\n",
       "       [ 6.70687437,  6.70687415],\n",
       "       [ 5.08859921,  5.08859903],\n",
       "       [15.34552383, 15.34552323],\n",
       "       [15.37119675, 15.37119598],\n",
       "       [ 9.90427494,  9.90427425],\n",
       "       [ 5.75682163,  5.75682169],\n",
       "       [ 7.43453979,  7.43454011],\n",
       "       [ 1.9716835 ,  1.97168342],\n",
       "       [ 0.65942436,  0.65942435],\n",
       "       [ 4.06076431,  4.06076417],\n",
       "       [ 0.62122703,  0.621227  ],\n",
       "       [ 0.7534191 ,  0.75341911],\n",
       "       [16.92503357, 16.92503181],\n",
       "       [11.87923527, 11.87923514],\n",
       "       [ 1.383371  ,  1.38337109],\n",
       "       [16.3419323 , 16.34193238],\n",
       "       [ 7.63263607,  7.63263566],\n",
       "       [ 4.57697392,  4.5769739 ],\n",
       "       [12.09562302, 12.09562338],\n",
       "       [ 9.70224094,  9.70224124],\n",
       "       [17.66047859, 17.66047928],\n",
       "       [ 5.52931929,  5.5293194 ]])"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate([custom_model.predict(x)[1],6*x+custom_model.predict(x)[0]],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.00117844],\n",
       "       [1.52394515],\n",
       "       [0.52260817],\n",
       "       [2.33294922],\n",
       "       [1.39929653],\n",
       "       [1.91182102],\n",
       "       [0.57568057],\n",
       "       [3.17875688],\n",
       "       [0.68931116],\n",
       "       [0.9623731 ]])"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x + custom_model.predict(x)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient of multi-dimensional function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CustomModel, self).__init__()\n",
    "#         self.input_layer = Lambda(lambda x: K.log(x[:,0:1]+2) + x[:,1:2]**2 * x[:,2:3] + x[:,2:3]**3  )\n",
    "#         self.input_layer = Lambda(lambda x: x[:,0:1] * x[:,1:2] * x[:,2:3]  )\n",
    "        self.input_layer = Lambda(lambda x:x[0]*x[1])\n",
    "#         self.grad_layer = Lambda(lambda x: K.gradients(x[0],x[1][:,0:1]))\n",
    "\n",
    "    def findGrad(self,func,argm):\n",
    "#         x_1 = self.input_copy[:,0:1]\n",
    "# #         x_2 = argm[1]\n",
    "# #         x_3 = argm[2]\n",
    "#         print(\"x_1_type is \",type(x_1))\n",
    "#         print(x_1)\n",
    "        return keras.layers.Lambda(lambda x: tf.gradients(x[0],x[1][0])) ([func,argm])\n",
    "#         return K.gradients(func,argm)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        input_1,input_2 = inputs\n",
    "        Z = self.input_layer([input_1,input_2])\n",
    "        Z_1 = self.findGrad(Z,inputs)\n",
    "        #     return self.dense2(x)\n",
    "        #     print(\"\\n\\n\\n this is the answer:\",self.square_layer(inputs))\n",
    "        #     print(\"hre is a break\")\n",
    "#         Z_2 = self.findGrad(Z_1,inputs)\n",
    "        return  Z_1\n",
    "\n",
    "\n",
    "custom_model = CustomModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1 = np.array([[0.],\n",
    "            [1.],\n",
    "            [2]])\n",
    "x_2 = np.array([[3.],\n",
    "            [4.],\n",
    "            [5]])\n",
    "# x = np.array([0.,1,2])\n",
    "# x = x[:,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[3.],\n",
       "        [4.],\n",
       "        [5.]], dtype=float32)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_val = custom_model.predict(x=[x_1,x_2])\n",
    "pred_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laplacian of multi-dimensional function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CustomModel, self).__init__()\n",
    "#         self.input_layer = Lambda(lambda x: K.log(x[:,0:1]+2) + x[:,1:2]**2 * x[:,2:3] + x[:,2:3]**3  )\n",
    "#         self.input_layer = Lambda(lambda x: x[:,0:1] * x[:,1:2] * x[:,2:3]  )\n",
    "#         self.input_layer = Lambda(lambda x: tf.sin(x[0]) + tf.square(x[1])*x[2] + tf.exp(x[2]) )\n",
    "        self.input_layer = Lambda(lambda x: tf.sin(x[:,0:1]) + tf.square(x[:,1:2])*x[:,2:3] + tf.exp(x[:,2:3]) )\n",
    "#         self.grad_layer = Lambda(lambda x: K.gradients(x[0],x[1][:,0:1]))\n",
    "\n",
    "\n",
    "    def findGrad(self,func,argm):\n",
    "        try:\n",
    "            return keras.layers.Lambda(lambda z: [tf.gradients(z[0],x_i,\n",
    "                                                               unconnected_gradients='zero')\n",
    "                                                  for x_i in z[1] ]) ([func,argm])\n",
    "        except Exception as e:\n",
    "            print(\"error occured in find gradient lambda layer of type {} as follows: \".format(type(e)),e)\n",
    "            \n",
    "            \n",
    "    def findSecGrad(self,func,argm):\n",
    "        try:\n",
    "            # list containng diagonal entries of hessian matrix. Note that  tf.gradients \n",
    "            #returns a list of tensors and hence thats why we have  a [0] at the end of  \n",
    "            #the tf.gradients fucntion as tf.gradients(func,argm) [0]\n",
    "            del_sq_layer = keras.layers.Lambda( lambda z: [ tf.gradients(z[0][i], z[1][i],\n",
    "                                                              unconnected_gradients='zero') [0]\n",
    "                                                  for i in range(len(z[1])) ] ) ([func,argm])\n",
    "            return sum(del_sq_layer)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(\"Error occured in find laplacian lambda layer of type {} as follows: \".format(type(e)),e)\n",
    "    \n",
    "    \n",
    "    def call(self, inputs):\n",
    "        input_1,input_2,input_3 = inputs\n",
    "#         Z = self.input_layer([input_1, input_2, input_3])\n",
    "        inputs_conc = keras.layers.concatenate(inputs)\n",
    "        Z = self.input_layer(inputs_conc)\n",
    "        Z_1 = self.findGrad(Z,inputs)\n",
    "        #     return self.dense2(x)\n",
    "        #     print(\"\\n\\n\\n this is the answer:\",self.square_layer(inputs))\n",
    "        #     print(\"hre is a break\")\n",
    "        Z_2 = self.findSecGrad(Z_1, inputs)\n",
    "        return  Z_2\n",
    "\n",
    "\n",
    "custom_model = CustomModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1 = np.array([[0.],\n",
    "            [1.],\n",
    "            [2],\n",
    "             [4.]  ], dtype=np.float32)\n",
    "x_2 = np.array([[3.],\n",
    "            [4.],\n",
    "            [5],\n",
    "              [6.] ],dtype=np.float32)\n",
    "x_3 = np.array([[9.],\n",
    "            [10.],\n",
    "            [16],\n",
    "             [15]  ],dtype=np.float32)\n",
    "# x = np.array([0.,1,2])\n",
    "# x = x[:,np.newaxis]\n",
    "X = np.concatenate([x_1,x_2,x_3], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.1210840e+03],\n",
       "       [2.2045623e+04],\n",
       "       [8.8861420e+06],\n",
       "       [3.2690482e+06]], dtype=float32)"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_val = custom_model.predict(x=[X[:,0:1],X[:,1:2],X[:,2:3]])\n",
    "pred_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 1), dtype=float32, numpy=\n",
       "array([[8.1210840e+03],\n",
       "       [2.2045623e+04],\n",
       "       [8.8861420e+06],\n",
       "       [3.2690480e+06]], dtype=float32)>"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-tf.sin(x_1)+ 2*x_3 + np.exp(x_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note for finding  gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Can use lambda layers to find gradient with respect to different dimension of input like x,y,z,t.\n",
    "2. If we feed the input as one single block of array(np,tf), then there is problem in using tf.gradients(function,input), since the returned gradient is some kind of sum of gradients in different dim. Please see the tf.gradients() manual.\n",
    "3. A work around will be to feed the different attributes(features) of the data as different inputs so we can find the partial derrivative with respect to each input dim without a problem. Please see the section on graddient of multi-dim function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regarding second gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Seems to be working fine except for some weird instances where None is returned and there is exception thrown while converting this None to a tensor.\n",
    "2. Key point being using the notes above to find the gradient with respect to different input separately and then finally use another lambda layer to find partial derrivative of each of the partial derrivative.\n",
    "3. Regarding the form of the del_sq layer in laplacian:   \n",
    "    Note that  tf.gradients \n",
    "    returns a list of tensors and hence thats why we have  a [0] at the end of  \n",
    "    the tf.gradients fucntion as tf.gradients(func,argm) [0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next thing to do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incorporate the lapace layer into the PINN with poission 2d problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
